<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lab 9: Mapping</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 40px;
            padding: 20px;
            background-color: #f4f4f4;
        }
        h1, h2, h3, h4 {
            color: #333;
        }
        .container {
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0px 0px 10px rgba(0,0,0,0.1);
        }
        p {
            line-height: 1.6;
        }
        img, iframe {
            width: 500px;
            height: auto;
            display: block;
            margin: 10px auto;
        }
        pre {
            background: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <div class="container">

<h1>Lab 9: Mapping</h1>

<h3>Objective</h3>
<p>
    The goal of this lab was to generate a 2D map of a static environment using a mobile robot equipped with Time-of-Flight (ToF) sensors and an Inertial Measurement Unit (IMU). This map will be used in future localization and navigation labs. The robot performed controlled rotations at several marked locations to collect distance data, which was transformed and merged into a global reference frame for visualization and map building.
</p>

<h3>Control</h3>
<p>
    I used PID orientation control to collect my data. Every second, I increased my target yaw by 25 degrees so that the robot would slowly turn in a circle in 14 increments. While the robot was turning, the ToF sensor continuously collected distance data. 
</p>
<p>
    The main challenge was tuning the K<sub>p</sub> value high enough to keep the robot from lagging too far behind the target, while avoiding overshoot. I found a balance where the robot would trail the target slightly, but overshoot was avoided. 
</p>

<h4>PID Orientation Control Graph:</h4>
<img src="PIDturn.png" alt="Graph showing PID target vs actual yaw and PWM over time">

<h4>PID-Controlled Turn Video:</h4>
<iframe width="500" height="300" src="https://www.youtube.com/embed/YOUR_VIDEO_ID" frameborder="0" allowfullscreen></iframe>

<h3>Error Estimation</h3>
<p>
    Given the reliability of my angular steps and the consistency of my PID controller, the primary error came from the robot not perfectly rotating on its center axis. I estimate the average drift to be about 3 inches (0.0762 m) and the maximum around 6 inches (0.1524 m). These are translational drifts, which result in small radial mapping errors.
</p>
<p>
    The error from this drift in a 4×4 m square room is relatively small—on the order of a few millimeters of radial error (1.5–5.8 mm). However, IMU drift presents a much larger problem. Assuming a 10° heading error, the positional mapping error can reach up to 34.7 cm. The graphs at the end of the scan show distortions that suggest both drift and slight heading misalignment.
</p>
<img src="drift_error.png" alt="Drift error estimation diagram">
<img src="imu_error.png" alt="IMU drift error estimation diagram">

<h3>Data Collection</h3>
<p>
    I collected distance and yaw data at four locations: (-3, -2), (5, 3), (0, 3), and (5, -3). At each location, the robot performed a full turn and sent the distance and orientation data to Python over Bluetooth. 
</p>
<p>
    In some cases, the starting yaw had to be manually adjusted in the plots because the robot’s startup orientation changed slightly after re-uploading code. This required aligning scans to maintain consistency.
</p>

<h4>Individual Scan Plots:</h4>
<img src="scan1.png" alt="Scan 1">
<img src="scan2.png" alt="Scan 2">
<img src="scan3.png" alt="Scan 3">
<img src="scan4.png" alt="Scan 4">

<h3>Rotational Consistency Check</h3>
<p>
    When performing multiple full turns, the shape of the room appeared consistent early on. However, once the PID controller lost control or the robot sped up, data quality deteriorated. Maintaining slower rotation improved consistency and reduced sensor noise.
</p>
<img src="multiturn_scan.png" alt="Multiple full turns scan result">

<h3>Transformations and Merging</h3>
<p>
    To map ToF readings into the global frame, I applied two transformations:
</p>

<h4>Transformation T₁: Sensor Frame → Robot Frame</h4>
<p>
    The ToF sensor is mounted 1.3 inches (0.03302 m) in front of the robot's center and aligned with its axis, so T₁ is a translation in the x-direction.
</p>
<img src="T1.png" alt="T1 transformation matrix">

<h4>Transformation T₂: Robot Frame → Inertial Frame</h4>
<p>
    T₂ includes a rotation by θ (robot's yaw) and a translation to the robot's global position (x<sub>r</sub>, y<sub>r</sub>).
</p>
<img src="T2.png" alt="T2 transformation matrix">

<h4>Sensor Measurement Vector P:</h4>
<img src="P_vector.png" alt="Sensor measurement vector">

<h4>Combined Transformation:</h4>
<p style="text-align: center;"><b>(x<sub>c</sub>, y<sub>c</sub>) = T₂ × T₁ × P</b></p>

<h3>Combined Scan Plot:</h3>
<img src="merged_map.png" alt="Merged map of all scans">

<h3>Line-Based Map Overlay:</h3>
<img src="line_overlay.png" alt="Line-based map overlay on scan data">

<br>
<a href="../index.html">Back to Main Page</a>

</div>
</body>
</html>

